{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of the trained BERT model\n",
    "#### Author: Rishikesh Kakde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikeshkakde/Library/CloudStorage/OneDrive-IndianaUniversity/SMM/Paper 3/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from empath import Empath\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Initialize the EMPATH tool\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset with Predicted Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "   rating                                              title  \\\n",
      "0     5.0                                         Five Stars   \n",
      "1     5.0                                         Five Stars   \n",
      "2     3.0                       Some decent moments...but...   \n",
      "3     4.0  Decent Depiction of Lower-Functioning Autism, ...   \n",
      "4     5.0                                    What Love Is...   \n",
      "\n",
      "                                                text images        asin  \\\n",
      "0           Amazon, please buy the show! I'm hooked!     []  B013488XFS   \n",
      "1                         My Kiddos LOVE this show!!     []  B00CB6VTDS   \n",
      "2  Annabella Sciorra did her character justice wi...     []  B096Z8Z3R6   \n",
      "3  ...there should be more of a range of characte...     []  B09M14D9FZ   \n",
      "4  ...isn't always how you expect it to be, but w...     []  B001H1SVZC   \n",
      "\n",
      "  parent_asin                       user_id      timestamp  helpful_vote  \\\n",
      "0  B013488XFS  AGGZ357AO26RQZVRLGU4D4N52DZQ  1440385637000             0   \n",
      "1  B00CB6VTDS  AGKASBHYZPGTEPO6LWZPVJWB2BVA  1461100610000             0   \n",
      "2  B096Z8Z3R6  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1646271834582             0   \n",
      "3  B09M14D9FZ  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1645937761864             1   \n",
      "4  B001H1SVZC  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1590639227074             0   \n",
      "\n",
      "   verified_purchase predicted_sentiment  \n",
      "0               True             neutral  \n",
      "1               True            positive  \n",
      "2               True            negative  \n",
      "3              False            positive  \n",
      "4               True             neutral  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'analysis_dataset_with_sentiments.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Dataset Preview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # Generate the Ground Truth Labels Using EMPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset with Ground Truth Labels:\n",
      "   rating                                              title  \\\n",
      "0     5.0                                         Five Stars   \n",
      "1     5.0                                         Five Stars   \n",
      "2     3.0                       Some decent moments...but...   \n",
      "3     4.0  Decent Depiction of Lower-Functioning Autism, ...   \n",
      "4     5.0                                    What Love Is...   \n",
      "\n",
      "                                                text images        asin  \\\n",
      "0           Amazon, please buy the show! I'm hooked!     []  B013488XFS   \n",
      "1                         My Kiddos LOVE this show!!     []  B00CB6VTDS   \n",
      "2  Annabella Sciorra did her character justice wi...     []  B096Z8Z3R6   \n",
      "3  ...there should be more of a range of characte...     []  B09M14D9FZ   \n",
      "4  ...isn't always how you expect it to be, but w...     []  B001H1SVZC   \n",
      "\n",
      "  parent_asin                       user_id      timestamp  helpful_vote  \\\n",
      "0  B013488XFS  AGGZ357AO26RQZVRLGU4D4N52DZQ  1440385637000             0   \n",
      "1  B00CB6VTDS  AGKASBHYZPGTEPO6LWZPVJWB2BVA  1461100610000             0   \n",
      "2  B096Z8Z3R6  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1646271834582             0   \n",
      "3  B09M14D9FZ  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1645937761864             1   \n",
      "4  B001H1SVZC  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1590639227074             0   \n",
      "\n",
      "   verified_purchase predicted_sentiment ground_truth_label  \n",
      "0               True             neutral            neutral  \n",
      "1               True            positive            neutral  \n",
      "2               True            negative           positive  \n",
      "3              False            positive           positive  \n",
      "4               True             neutral           positive  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate the ground truth label based on the text\n",
    "def assign_ground_truth_label(text):\n",
    "    if not text:\n",
    "        return None  # Handle empty or missing text\n",
    "    analysis = lexicon.analyze(text, categories=['positive_emotion', 'negative_emotion'])\n",
    "    positive_score = analysis.get('positive_emotion', 0)\n",
    "    negative_score = analysis.get('negative_emotion', 0)\n",
    "    \n",
    "    if positive_score > negative_score:\n",
    "        return 'positive'\n",
    "    elif negative_score > positive_score:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply the function to the text column to create the 'ground_truth_label' column\n",
    "df['ground_truth_label'] = df['text'].apply(assign_ground_truth_label)\n",
    "\n",
    "# Drop rows where ground truth could not be assigned\n",
    "df = df.dropna(subset=['ground_truth_label'])\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"Updated Dataset with Ground Truth Labels:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Sentiment Labels to Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped Sentiment Labels:\n",
      "   predicted_label  ground_truth_label\n",
      "0                1                   1\n",
      "1                2                   1\n",
      "2                0                   2\n",
      "3                2                   2\n",
      "4                1                   2\n"
     ]
    }
   ],
   "source": [
    "# Define mappings for both predicted and ground truth sentiments\n",
    "sentiment_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "\n",
    "# Map the sentiments to numeric labels\n",
    "df['predicted_label'] = df['predicted_sentiment'].map(sentiment_mapping)\n",
    "df['ground_truth_label'] = df['ground_truth_label'].map(sentiment_mapping)\n",
    "\n",
    "# Drop rows with invalid mappings (if any)\n",
    "df = df.dropna(subset=['predicted_label', 'ground_truth_label'])\n",
    "\n",
    "# Display the numeric labels\n",
    "print(\"Mapped Sentiment Labels:\")\n",
    "print(df[['predicted_label', 'ground_truth_label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.80\n",
      "Precision: 0.80\n",
      "Recall: 0.80\n",
      "F1-Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Extract the true and predicted labels\n",
    "y_true = df['ground_truth_label']\n",
    "y_pred = df['predicted_label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy and Metrics Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class     Precision Recall    F1-Score  Support   Accuracy  \n",
      "negative  0.60      0.66      0.63      213       0.66      \n",
      "neutral   0.88      0.80      0.84      767       0.80      \n",
      "positive  0.78      0.84      0.81      520       0.84      \n"
     ]
    }
   ],
   "source": [
    "# Use precision_recall_fscore_support to get metrics for each class\n",
    "class_metrics = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0, 1, 2])\n",
    "\n",
    "# Extract precision, recall, F1-score, and support for each class\n",
    "precision_per_class, recall_per_class, f1_per_class, support_per_class = class_metrics\n",
    "\n",
    "# Calculate accuracy per class\n",
    "# Accuracy per class = Correct predictions / Total samples in that class\n",
    "class_accuracies = []\n",
    "for cls in range(3):  # For each class (0: negative, 1: neutral, 2: positive)\n",
    "    class_total = support_per_class[cls]  # Total samples in the class\n",
    "    correct_predictions = sum((y_true == cls) & (y_pred == cls))  # Correct predictions for the class\n",
    "    class_accuracy = correct_predictions / class_total if class_total > 0 else 0\n",
    "    class_accuracies.append(class_accuracy)\n",
    "\n",
    "# Display per-class metrics\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "print(f\"{'Class':<10}{'Precision':<10}{'Recall':<10}{'F1-Score':<10}{'Support':<10}{'Accuracy':<10}\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"{label:<10}{precision_per_class[i]:<10.2f}{recall_per_class[i]:<10.2f}{f1_per_class[i]:<10.2f}{support_per_class[i]:<10}{class_accuracies[i]:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision, recall, F1 scores are all mediocre due to less trainig. With more compute and RAM available this model can be fine-tuned more to have better performnce metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
